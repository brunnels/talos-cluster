---
# yaml-language-server: $schema=https://kube-schemas.pages.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app vllm-stack
spec:
  chart:
    spec:
      chart: *app
      version: 0.1.7
      sourceRef:
        kind: HelmRepository
        name: *app
  interval: 1h
  install:
    timeout: 1h
    crds: CreateReplace
    strategy:
      name: RetryOnFailure
  rollback:
    cleanupOnFail: true
    recreate: true
  upgrade:
    timeout: 1h
    cleanupOnFail: true
    crds: CreateReplace
    strategy:
      name: RemediateOnFailure
    remediation:
      remediateLastFailure: true
      retries: 2
  values:
    servingEngineSpec:
      runtimeClassName: nvidia
      modelSpec:
        - name: llama3
          repository: lmcache/vllm-openai
          tag: v0.3.8
          modelURL: meta-llama/Llama-3.1-8B-Instruct
          replicaCount: 2

          requestCPU: 6
          requestMemory: 16Gi
          requestGPU: 1

          pvcStorage: 50Gi
          pvcAccessMode:
            - ReadWriteOnce
          storageClass: local-path

          vllmConfig:
            enablePrefixCaching: true
            maxModelLen: 4096
            dtype: bfloat16
            extraArgs:
              - --disable-log-requests
              - --gpu-memory-utilization
              - 0.8

          lmcacheConfig:
            enabled: true
            cpuOffloadingBufferSize: 20
          hf_token:
            secret_name: *app
            secret_key: HF_TOKEN

    cacheserverSpec:
      replicaCount: 1
      containerPort: 8080
      servicePort: 81
      serde: naive
      repository: lmcache/vllm-openai
      tag: v0.3.8
      resources:
        requests:
          cpu: 2
          memory: 2G
        limits:
          cpu: 2
          memory: 10G
      labels:
        environment: cacheserver
        release: cacheserver
